{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow\n",
    "# !pip install tf-nightly\n",
    "# !pip install pandas\n",
    "# !pip install ibm_boto3\n",
    "# !pip install botocore\n",
    "# !pip install ibm-cos-sdk\n",
    "# !pip install imageio\n",
    "# !pip install seaborn\n",
    "# !pip install plotly\n",
    "# !pip install tensorflow_hub\n",
    "# !pip install edge_ai_model_mgmt\n",
    "# !pip install tensorflow-model-optimization\n",
    "# !pip install edge_ai_model_mgmt\n",
    "# !pip install edgeaimodelmgmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "# import cv2\n",
    "import os, types\n",
    "import math\n",
    "import imageio\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from IPython.display import SVG\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from tensorflow.keras.applications import VGG16,inception_v3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n",
    "from tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n",
    "                                    Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n",
    "                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n",
    "                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_model_optimization as tfmot\n",
    "# from edgeaimodelmgmt.ode.tf_pruner import TfPruner\n",
    "# from edgeaimodelmgmt.ode.tf_quantizer import TfQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.4.1\n",
      "Hub version: 0.12.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 18:13:28.516821: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'gurvsin3-visualrecognition'\n",
    "object_key = 'fire_dataset.zip'\n",
    "\n",
    "DATA_DIR = os.path.join(os.path.abspath(os.path.pardir),'data')\n",
    "RESULTS_DIR = os.path.join('../results/fire_detection', 'tfhub')\n",
    "dataZip = os.path.join(DATA_DIR, object_key)\n",
    "dataset_dir = pathlib.Path(DATA_DIR+'/fire_dataset')\n",
    "\n",
    "batch_size = 32\n",
    "IMAGE_SIZE = (224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "    'IBM_API_KEY_ID': '816HFeHbtI_WiIlkFTaAO61vld7bM7nDB08U7MpaVHNm',\n",
    "    'COS_SERVICE_CRN': 'crn:v1:bluemix:public:iam-identity::a/2c303e22de2b34062121bab41b176b1b::serviceid:ServiceId-3152b935-e74b-4e7d-8460-6e9d56d166e8',\n",
    "    'ENDPOINT': 'https://s3.jp-tok.cloud-object-storage.appdomain.cloud',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/identity/token',\n",
    "    'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h',\n",
    "    'FILE': 'fire_dataset.zip'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataFromCOS(bucket_name, object_key):\n",
    "    \n",
    "    def __iter__(self): return 0\n",
    "\n",
    "    cos_cli = ibm_boto3.client(service_name='s3',\n",
    "        ibm_api_key_id=credentials_1[\"IBM_API_KEY_ID\"],\n",
    "        ibm_service_instance_id=credentials_1[\"COS_SERVICE_CRN\"],\n",
    "        ibm_auth_endpoint=credentials_1[\"IBM_AUTH_ENDPOINT\"],\n",
    "        config=Config(signature_version=\"oauth\"),\n",
    "        endpoint_url=credentials_1[\"ENDPOINT\"]\n",
    "    )\n",
    "\n",
    "    # Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "    # Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "    # ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "    # pandas documentation: http://pandas.pydata.org/\n",
    "    streaming_body_1 = cos_cli.get_object(Bucket=bucket_name, Key=object_key)['Body']\n",
    "    # add missing __iter__ method, so pandas accepts body as file-like object\n",
    "    if not hasattr(streaming_body_1, \"__iter__\"): streaming_body_1.__iter__ = types.MethodType( __iter__, streaming_body_1 ) \n",
    "        \n",
    "    f = open(object_key, 'wb')\n",
    "    f.write(streaming_body_1.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_dataset(file_path):\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ZIP file exists\n",
      "Dataset unziped Exists\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    if os.path.exists(object_key):\n",
    "        print('Data ZIP file exists')\n",
    "        if(os.path.isdir(DATA_DIR)):\n",
    "            print('Dataset unziped Exists')\n",
    "        else:\n",
    "            print('Unzip required..')\n",
    "            unzip_dataset(object_key)\n",
    "    else:\n",
    "        print('No Data')\n",
    "        loadDataFromCOS(bucket_name, object_key)\n",
    "        unzip_dataset(object_key)\n",
    "\n",
    "load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT ONLY TO REMOVE FILES\n",
    "# shutil.rmtree(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = pathlib.Path(data_dir_path+'/fire_dataset')\n",
    "jpg_image_count = len(list(dataset_dir.glob('*/*.jpg')))\n",
    "png_image_count = len(list(dataset_dir.glob('*/*.png')))\n",
    "print(jpg_image_count, png_image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_images = list(dataset_dir.glob('fire/*'))\n",
    "PIL.Image.open(str(fire_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fire_images = list(dataset_dir.glob('non_fire/*'))\n",
    "PIL.Image.open(str(non_fire_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: efficientnetv2-b0-21k : https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\n",
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"efficientnetv2-b0-21k\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1650 files belonging to 2 classes.\n",
      "Using 1320 files for training.\n",
      "Found 1650 files belonging to 2 classes.\n",
      "Using 330 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 18:13:43.876483: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(subset):\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        dataset_dir,\n",
    "        validation_split=.20,\n",
    "        subset=subset,\n",
    "        label_mode=\"categorical\",\n",
    "        # Seed needs to provided when using validation_split and shuffle = True.\n",
    "        # A fixed seed is used so that the validation set is stable across runs.\n",
    "        seed=123,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=1)\n",
    "\n",
    "train_ds = build_dataset(\"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "do_data_augmentation = False\n",
    "if do_data_augmentation:\n",
    "    preprocessing_model.add(\n",
    "      tf.keras.layers.experimental.preprocessing.RandomRotation(40))\n",
    "    preprocessing_model.add(\n",
    "      tf.keras.layers.experimental.preprocessing.RandomTranslation(0, 0.2))\n",
    "    preprocessing_model.add(\n",
    "      tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0))\n",
    "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
    "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
    "  # If all training inputs are larger than image_size, one could also use\n",
    "  # RandomCrop with a batch size of 1 and rebatch later.\n",
    "    preprocessing_model.add(\n",
    "      tf.keras.layers.experimental.preprocessing.RandomZoom(0.2, 0.2))\n",
    "    preprocessing_model.add(\n",
    "      tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"))\n",
    "    \n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds = build_dataset(\"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(validation_split=0.2, \n",
    "#                              rescale=1./255,\n",
    "#                              preprocessing_function=preprocess_input)\n",
    "# train = datagen.flow_from_directory(data_dir,\n",
    "#                                               target_size=(img_width, img_height), color_mode='rgb',\n",
    "#                                               class_mode='binary',shuffle=True, batch_size=32,subset=\"training\" )\n",
    "# valid = datagen.flow_from_directory(data_dir,\n",
    "#                                               target_size=(img_width, img_height), color_mode='rgb',\n",
    "#                                               class_mode='binary',shuffle=True, batch_size=8,subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "#     model = Sequential([\n",
    "#       layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "#       layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "#       layers.MaxPooling2D(),\n",
    "#       layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "#       layers.MaxPooling2D(),\n",
    "#       layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "#       layers.MaxPooling2D(),\n",
    "#       layers.Flatten(),\n",
    "#       layers.Dense(128, activation='relu'),\n",
    "#       layers.Dense(num_classes)\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    do_fine_tuning = False\n",
    "    print(\"Building model with\", model_handle)\n",
    "    INPUT_SHAPE = (None, 224, 224, 3)\n",
    "    model = tf.keras.Sequential([\n",
    "        # Explicitly define the input shape so the model can be properly\n",
    "        # loaded by the TFLiteConverter\n",
    "        layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "        hub.KerasLayer(model_handle, trainable=do_fine_tuning, input_shape=INPUT_SHAPE),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Dense(len(class_names),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "    ])\n",
    "    model.build((None,)+IMAGE_SIZE+(3,))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_4 (KerasLayer)   (None, 1280)              5919312   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 5,921,874\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# model.compile(\n",
    "#   optimizer=tf.keras.optimizers.Adam(lr=0.005),\n",
    "#   loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "#   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "41/41 [==============================] - 84s 2s/step - loss: 0.8882 - accuracy: 0.8485 - val_loss: 0.4192 - val_accuracy: 0.9688\n",
      "Epoch 2/15\n",
      "41/41 [==============================] - 76s 2s/step - loss: 0.4142 - accuracy: 0.9465 - val_loss: 0.3214 - val_accuracy: 0.9625\n",
      "Epoch 3/15\n",
      "41/41 [==============================] - 76s 2s/step - loss: 0.3821 - accuracy: 0.9450 - val_loss: 0.3780 - val_accuracy: 0.9375\n",
      "Epoch 4/15\n",
      "41/41 [==============================] - 76s 2s/step - loss: 0.3530 - accuracy: 0.9517 - val_loss: 0.4167 - val_accuracy: 0.9625\n",
      "Epoch 5/15\n",
      "41/41 [==============================] - 78s 2s/step - loss: 0.3941 - accuracy: 0.9587 - val_loss: 0.3663 - val_accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,mode='auto')\n",
    "\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = valid_size // BATCH_SIZE\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=15, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[Callback_Stop_Early],\n",
    "    validation_steps=validation_steps).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(history):\n",
    "    print(history.keys())\n",
    "    sns.set()\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_graph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/fire_detection/tfhub/fire_tfhub.h5\n"
     ]
    }
   ],
   "source": [
    "model_save_path = os.path.join(RESULTS_DIR, 'fire_tfhub.h5')\n",
    "print(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              5919312   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 5,921,874\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "model = load_model(model_save_path, custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.saved_model.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lite(model):\n",
    "    optimize_lite_model = False \n",
    "    num_calibration_examples = 60 \n",
    "    representative_dataset = None\n",
    "    if optimize_lite_model and num_calibration_examples:\n",
    "      # Use a bounded number of training examples without labels for calibration.\n",
    "      # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
    "        representative_dataset = lambda: itertools.islice(\n",
    "            ([image[None, ...]] for batch, _ in train_ds for image in batch),\n",
    "            num_calibration_examples)\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    if optimize_lite_model:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        if representative_dataset:  # This is optional, see above.\n",
    "            converter.representative_dataset = representative_dataset\n",
    "    lite_model_content = converter.convert()\n",
    "\n",
    "    with open(RESULTS_DIR+'/model.tflite', \"wb\") as f:\n",
    "        f.write(lite_model_content)\n",
    "    print(\"Wrote %sTFLite model of %d bytes.\" %\n",
    "          (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))\n",
    "\n",
    "convert_to_lite(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_ds))\n",
    "image = x[0, :, :, :]\n",
    "true_index = np.argmax(y[0])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(np.expand_dims(image, axis=0).shape)\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "print('prediction_scores: ', prediction_scores)\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + class_names[true_index])\n",
    "print(\"Predicted label: \" + class_names[predicted_index])\n",
    "\n",
    "# loss, acc = model.evaluate(val_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../data/datasets/test_img\n",
    "class_names = ['fire', 'non_fire']\n",
    "\n",
    "# test_img_url = 'https://climate.esa.int/media/images/Fire-CCI-banner_1.2e16d0ba.fill-600x314-c100.format-jpeg.jpg'\n",
    "# test_img_url = 'https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/small-kitchen-1572367025.png'\n",
    "# test_img_url = 'https://images.indianexpress.com/2021/03/Pune-fire-1.jpg'\n",
    "# test_img_url = 'https://i.insider.com/56843d15dd0895dc648b47fd?width=1000&format=jpeg&auto=webp'\n",
    "# test_img_url = 'https://charlestonfire.org/images/public-news/large/news-1501509679.jpg' \n",
    "# test_img_url = 'https://cf.ltkcdn.net/safety/images/std/116578-232x317r1-Kitchenfireprevention.jpg' # No_Fire\n",
    "# test_img_url ='https://www.kutchina.com/wp-content/uploads/2020/07/straight-line-2-min-600x350.jpg' # No_Fire\n",
    "# test_img_url = 'https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/kitchen-ideas-wall-display-1603743563.jpg'\n",
    "# test_img_url = 'https://media.springernature.com/lw725/springer-cms/rest/v1/content/19831396/data/v1' # False Negative\n",
    "# test_img_url = 'https://media-exp1.licdn.com/dms/image/C4D1BAQGrF7AoQDBPXQ/company-background_10000/0/1631550860606?e=2147483647&v=beta&t=0MXoiDfXevwDLrZ8ZHFEXxlcS0T_iyMShpgHlunLMQo'\n",
    "# test_img_url = 'https://www.thespruce.com/thmb/XMuqYgCjvYclSkDrm7ALrwnoXw8=/1777x1333/smart/filters:no_upscale()/fire-pit-backyard-ideas-4177680-hero-4b2d98b9204b4cd4bdb534a5a9b106f4.jpg' # False Negative\n",
    "\n",
    "test_img_url = 'https://cdn.britannica.com/49/161649-050-3F458ECF/Bernese-mountain-dog-grass.jpg?q=60'\n",
    "\n",
    "test_img_path = tf.keras.utils.get_file('test_img', origin=test_img_url, cache_dir='../data')\n",
    "# test_img_path = '/Users/gurvindersingh/Downloads/fire.jpeg'\n",
    "\n",
    "# test_img_path = '../data/fire_dataset/non_fire/10.jpg'\n",
    "\n",
    "print(test_img_path)\n",
    "\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    test_img_path, target_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(test_img_path).convert('RGB').resize(IMAGE_SIZE, Image.ANTIALIAS)\n",
    "# img_array = np.float32(image) / 255\n",
    "img_array = keras.preprocessing.image.img_to_array(img) / 255\n",
    "# img_array = (np.float32(image) - 123) / 123\n",
    "# img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "# img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "predictions = model.predict(np.expand_dims(img_array, axis=0))\n",
    "print(\"predictions: \", predictions)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "predicted_index = np.argmax(predictions)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "# print('score: ', score)\n",
    "print('Class: ', np.argmax(score))\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=RESULTS_DIR+'/model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "_, height, width, _ = interpreter.get_input_details()[0]['shape']\n",
    "print('width: ', width, ', height: ', height)\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "output_index = output_details[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_tensor(interpreter, image):\n",
    "    tensor_index = interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "def classify_image(interpreter, image, top_k=1):\n",
    "    \"\"\"Returns a sorted array of classification results.\"\"\"\n",
    "#     set_input_tensor(interpreter, image)\n",
    "    interpreter.set_tensor(input_index, image)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "#     output = np.squeeze(interpreter.get_tensor(output_index['index']))\n",
    "    output = interpreter.tensor(output_index)\n",
    "#     digit = np.argmax(output()[0])\n",
    "#     confidence = np.max(output()[0])\n",
    "    score = (np.exp(output()).T / np.exp(output()).sum(axis=-1)).T\n",
    "    print('score', score)\n",
    "    # If the model is quantized (uint8 data), then dequantize the results\n",
    "    if output_details['dtype'] == np.uint8:\n",
    "        scale, zero_point = output_details['quantization']\n",
    "        output = scale * (output - zero_point)\n",
    "        print('quantized model output: >> ', output)\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(test_img_path).convert('RGB').resize(IMAGE_SIZE, Image.ANTIALIAS)\n",
    "img_array = np.float32(image) / 255\n",
    "\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "print(img_array.shape)\n",
    "\n",
    "result = classify_image(interpreter, img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from edgeaimodelmgmt.ode.tf_pruner import TfPruner\n",
    "from edgeaimodelmgmt.ode.tf_quantizer import TfQuantizer\n",
    "from modelmgmt.fingerprinting.model_fingerprint import ModelFingerprint\n",
    "from modelmgmt.fingerprinting.fingerprint import Fingerprint\n",
    "from modelmgmt.models.keras_model import KerasModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 18:15:03.148133: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i, (image, label) in enumerate(train_ds.unbatch().take(1320)):\n",
    "    x_train.append(image.numpy())\n",
    "    y_train.append(label.numpy())\n",
    "    \n",
    "for i, (image, label) in enumerate(val_ds.unbatch().take(330)):\n",
    "    x_test.append(image.numpy())\n",
    "    y_test.append(label.numpy())\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1320, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              5919312   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 5,921,874\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# custom_objects={'KerasLayer':hub.KerasLayer}\n",
    "\n",
    "# with tfmot.quantization.keras.quantize_scope({'KerasLayer':hub.KerasLayer}):\n",
    "#     loaded_model = load_model(model_save_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:edgeaimodelmgmt.ode.tf_quantizer:Could not quantize the model due to: Cannot assign to variable efficientnetv2-b0/blocks_0/conv2d/kernel:0 due to variable shape (3, 3, 32, 16) and value shape (1, 1, 192, 1280) are incompatible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not quantize the model...\n"
     ]
    }
   ],
   "source": [
    "q = TfQuantizer(model)\n",
    "\n",
    "with tfmot.quantization.keras.quantize_scope({'KerasLayer':hub.KerasLayer}):\n",
    "    q_aware_model, err = q.quantize(np.int8, 8, x_train, y_train, x_test, y_test, \n",
    "                     include_biases=False, optimizer=tf.keras.optimizers.SGD)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "if not err:\n",
    "    loss, acc = q_aware_model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"Quant trained model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "else:\n",
    "    print(\"Could not quantize the model...\")\n",
    "#     exit() # terminate execution as we cannot continue with the script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'X-IBM-Client-Id': '4ada90b8-88ec-4ef5-88a5-2c15df158069',\n",
    "           'X-IBM-Client-Secret':'V0hT7sL5yN5cK5sG3jF5fF5cL0rR0kC8lX4oA7yM8gC5rC4hB5'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfilename = 'fire.pickle'\n",
    "\n",
    "with open(dsfilename, 'wb') as file:\n",
    "    pickle.dump(x_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.ibm.com/edgeai/run/modelmgmt/generate_fingerprint' # your url may differ. change as neccessary\n",
    "\n",
    "data = {'model_type': 'keras', 'partial_activations': 0.2}\n",
    "\n",
    "multiple_files = [\n",
    "    ('dataset', ('fire.pickle', open(dsfilename, 'rb'), 'file/pickle')),\n",
    "    ('model', ('fire.h5', open(model_save_path, 'rb'), 'file/h5'))]\n",
    "\n",
    "r = requests.post(url, data=data, files=multiple_files, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()\n",
    "task_id = r.json()['task_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.ibm.com/edgeai/run/modelmgmt/status/{}'.format(task_id)\n",
    "\n",
    "r = requests.get(url, headers=headers, verify=False)\n",
    "r.json()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
